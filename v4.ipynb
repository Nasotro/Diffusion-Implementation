{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from models.unet import Unet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils.utils import show_image, CFG\n",
    "from utils.noise import CosineNoiseAdder\n",
    "\n",
    "from data.dataset import MNIST_Dataset, CIFAR10_Dataset, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start with shape torch.Size([1, 3, 128, 128])\n",
      "after concatenating the timestep (& labels) embedds : torch.Size([1, 67, 128, 128])\n",
      "down block 0, with shape torch.Size([1, 67, 128, 128])\n",
      "down block 1, with shape torch.Size([1, 64, 64, 64])\n",
      "down block 2, with shape torch.Size([1, 128, 32, 32])\n",
      "after bottleneck : shape = torch.Size([1, 256, 32, 32])\n",
      "up block 0, with shape torch.Size([1, 256, 32, 32]), and skip shape : torch.Size([1, 256, 32, 32])\n",
      "up block 1, with shape torch.Size([1, 128, 64, 64]), and skip shape : torch.Size([1, 128, 64, 64])\n",
      "after final : shape = torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# testing net shapes\n",
    "net = Unet(64, depth=3, embed_dim=64, initial_channels=3, conv_layers=3)\n",
    "test_img = torch.randn(1, 3, 128, 128)\n",
    "test_time = torch.tensor([1])\n",
    "test_label = torch.tensor([1])\n",
    "print(net(test_img, test_time, test_label, verbose=True).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseDataset():\n",
    "    def __init__(self, imgs_dataset, noise_schedule = None):\n",
    "        self.imgs_dataset = imgs_dataset\n",
    "        self.noise_schedule = noise_schedule if noise_schedule else CosineNoiseAdder()       \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.imgs_dataset[idx]\n",
    "        t = torch.randint(self.noise_schedule.T, (1, )).squeeze()\n",
    "        noisy_img, noise = self.noise_schedule.image_at_time_step(img, t)\n",
    "        return noisy_img, noise, t, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model:nn.Module, _test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for i, (noisy_imgs, noises, time_steps, labels) in enumerate(_test_loader):\n",
    "            noisy_imgs, noises, time_steps, labels = noisy_imgs.to(device), noises.to(device), time_steps.to(device), labels.to(device)\n",
    "            outputs = model(noisy_imgs, time_steps, labels)\n",
    "            loss = criterion(outputs, noises)\n",
    "            losses.append(loss.item())\n",
    "    return sum(losses)/len(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(cfg:CFG, model, train, test, device=None):\n",
    "    if device is None:\n",
    "        # Set the device to GPU if available, else CPU\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model.apply(init_weights)  # Initialize model weights\n",
    "\n",
    "    train_loader = DataLoader(train, shuffle=True, batch_size=cfg.batch_size)\n",
    "    test_loader = DataLoader(test, shuffle=False, batch_size=cfg.batch_size)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=cfg.n_epochs_lr * len(train_loader), eta_min=cfg.final_lr)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # keep track of the loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_loss = np.inf\n",
    "    best_loss_i = 0\n",
    "    stoping = False\n",
    "    eval_every = int(len(train_loader) * cfg.eval_frequency) # evaluate every n% of the training set\n",
    "\n",
    "    # Compile the model for better performance\n",
    "    if cfg.use_compile:\n",
    "        print(\"Compiling the model...\")\n",
    "        torch.compile(model, mode='default', dynamic=True)\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(cfg.n_epochs):\n",
    "        if stoping: # if the early stopping is triggered, we stop the training\n",
    "            break\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f'Epoch {epoch+1}/{cfg.n_epochs}', leave=True)\n",
    "        for i, batch in enumerate(train_loader_tqdm):\n",
    "            noisy_imgs, noises, time_steps, labels = batch\n",
    "            noisy_imgs, noises, time_steps, labels = noisy_imgs.to(device, dtype=cfg.images_precision), noises.to(device, dtype=cfg.images_precision), time_steps.to(device, dtype=torch.int16), labels.to(device, dtype=torch.int32)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predicted_noise = model(noisy_imgs, time_steps, labels, verbose=False)\n",
    "            loss = criterion(predicted_noise, noises)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # evaluate the model every eval_every steps\n",
    "            if i % eval_every == 0 and i > 0:\n",
    "                \n",
    "                val_loss = eval_model(model, test_loader, criterion, device)\n",
    "                val_losses.append(val_loss)\n",
    "                # print(f'Epoch [{epoch+1}/{n_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, lr: {scheduler.get_last_lr()[0]}')\n",
    "\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    best_loss_i = epoch * len(train_loader) + i\n",
    "                    torch.save(model.state_dict(), cfg.save_path)\n",
    "\n",
    "                train_loader_tqdm.set_postfix({'Loss': loss.item(), 'Val Loss': val_loss, 'best_loss' : best_loss, 'lr': scheduler.get_last_lr()[0]})\n",
    "                \n",
    "                if epoch * len(train_loader) + i - best_loss_i > cfg.patience:\n",
    "                    print(\"Stopping early\")\n",
    "                    stoping = True\n",
    "                    break\n",
    "                \n",
    "            if epoch < cfg.n_epochs_lr:\n",
    "                scheduler.step()\n",
    "        \n",
    "        if stoping:\n",
    "            break\n",
    "\n",
    "\n",
    "    # plot the losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    train_losses_resized = [np.mean(train_losses[i * eval_every:(i + 1) * eval_every]) for i in range(len(val_losses))]\n",
    "    plt.plot(np.arange(0, len(train_losses_resized)) * eval_every / len(train_loader), train_losses_resized, label='Training Loss (averaged)')\n",
    "    plt.plot(np.arange(0, len(val_losses)) * eval_every / len(train_loader), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "config_CELEBA = CFG(\n",
    "    dataset_name='CELEBA',\n",
    "    save_path = 'best_model_CELEBA.pth',\n",
    "    initial_channels=3,\n",
    "    image_size=64,\n",
    "    n_epochs=30,\n",
    "    num_labels=0,\n",
    "    \n",
    "    lr=1e-3,\n",
    "    final_lr=1e-5,\n",
    "    n_epochs_lr=25,\n",
    "    batch_size=32,\n",
    "    patience=10000,\n",
    "    \n",
    "    depth=3,\n",
    "    conv_layers=2,\n",
    "    first_hidden=64,\n",
    "    embedding_dim=64,\n",
    "    max_time_steps=400,\n",
    ")\n",
    "\n",
    "CosineNoise = CosineNoiseAdder(config_CELEBA.max_time_steps)\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((config_CELEBA.image_size, config_CELEBA.image_size)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5, ), (0.5, 0.5, 0.5, ))\n",
    "])\n",
    "\n",
    "train = NoiseDataset(Dataset(config_CELEBA.dataset_name, transform=trans), CosineNoise)\n",
    "test = NoiseDataset(Dataset(config_CELEBA.dataset_name, 'test', transform=trans), CosineNoise)\n",
    "\n",
    "model = Unet(\n",
    "    first_hidden=config_CELEBA.first_hidden, depth=config_CELEBA.depth, embed_dim=config_CELEBA.embedding_dim, \n",
    "    num_label=config_CELEBA.num_labels, initial_channels=config_CELEBA.initial_channels, \n",
    "    conv_layers=config_CELEBA.conv_layers, dropout=config_CELEBA.dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CFG' object has no attribute 'to_yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mconfig_CELEBA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_yaml\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigs/config_CELEBA.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CFG' object has no attribute 'to_yaml'"
     ]
    }
   ],
   "source": [
    "config_CELEBA.to_yaml('configs/config_CELEBA.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   8%|â–Š         | 418/5087 [04:54<54:45,  1.42it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_CELEBA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(cfg, model, train, test, device)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     36\u001b[0m train_loader_tqdm \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader_tqdm):\n\u001b[0;32m     38\u001b[0m     noisy_imgs, noises, time_steps, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     39\u001b[0m     noisy_imgs, noises, time_steps, labels \u001b[38;5;241m=\u001b[39m noisy_imgs\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mimages_precision), noises\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mimages_precision), time_steps\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint16), labels\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\n",
      "File \u001b[1;32mc:\\Users\\lorra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lorra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\lorra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\lorra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\lorra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mNoiseDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m----> 7\u001b[0m     img, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgs_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_schedule\u001b[38;5;241m.\u001b[39mT, (\u001b[38;5;241m1\u001b[39m, ))\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m      9\u001b[0m     noisy_img, noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_schedule\u001b[38;5;241m.\u001b[39mimage_at_time_step(img, t)\n",
      "File \u001b[1;32mc:\\Users\\lorra\\Documents\\Projets\\AI\\Remake Papers\\Diffusion\\data\\dataset.py:44\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx:\u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a tuple of (image, label)\"\"\"\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lorra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\celeba.py:160\u001b[0m, in \u001b[0;36mCelebA.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[1;32m--> 160\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg_align_celeba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     target: Any \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_type:\n",
      "File \u001b[1;32mc:\\Users\\lorra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3215\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3218\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3219\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_model(config_CELEBA, model, train, test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (time_emb): TimeEmbedding(\n",
       "    (time_mlp): Sequential(\n",
       "      (0): SinusoidalPositionEmbeddings()\n",
       "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (2): SiLU()\n",
       "      (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (label_emb): LabelEmbedding(\n",
       "    (emb): Embedding(10, 64)\n",
       "    (proj): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): DownBlock(\n",
       "      (silu): SiLU()\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(67, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1-2): 2 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (norms): ModuleList(\n",
       "        (0-2): 3 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropouts): ModuleList(\n",
       "        (0-2): 3 x Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (max_pooling): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): DownBlock(\n",
       "      (silu): SiLU()\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1-2): 2 x Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (norms): ModuleList(\n",
       "        (0-2): 3 x BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropouts): ModuleList(\n",
       "        (0-2): 3 x Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (max_pooling): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): DownBlock(\n",
       "      (silu): SiLU()\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1-2): 2 x Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (norms): ModuleList(\n",
       "        (0-2): 3 x BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropouts): ModuleList(\n",
       "        (0-2): 3 x Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (max_pooling): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): UpBlock(\n",
       "      (silu): SiLU()\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1-2): 2 x Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (norms): ModuleList(\n",
       "        (0-2): 3 x BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropouts): ModuleList(\n",
       "        (0-2): 3 x Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (up_conv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (1): UpBlock(\n",
       "      (silu): SiLU()\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1-2): 2 x Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (norms): ModuleList(\n",
       "        (0-2): 3 x BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropouts): ModuleList(\n",
       "        (0-2): 3 x Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (up_conv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (bottleneck): BottleNeck(\n",
       "    (convs): ModuleList(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1-2): 2 x Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (norms): ModuleList(\n",
       "      (0-2): 3 x BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (dropouts): ModuleList(\n",
       "      (0-2): 3 x Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (up_conv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (silu): SiLU()\n",
       "  )\n",
       "  (final): FinalBlock(\n",
       "    (silu): SiLU()\n",
       "    (convs): ModuleList(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1-2): 2 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (norms): ModuleList(\n",
       "      (0-2): 3 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (dropouts): ModuleList(\n",
       "      (0-2): 3 x Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (final_conv): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(config_CELEBA.save_path))\n",
    "net.to(device)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, max_time_steps, n_samples=1, config=None, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    CosineNoise = CosineNoiseAdder(max_time_steps, s=0.008)\n",
    "    all_samples = []\n",
    "    all_vis = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        with torch.no_grad():\n",
    "            full_img = torch.tensor([], device=device)\n",
    "            full_predicted_noise = torch.tensor([], device=device)\n",
    "        \n",
    "            xt = torch.randn((1, config.initial_channels, 32, 32)).to(device)\n",
    "            for t in torch.arange(max_time_steps-1, -1, -1):\n",
    "                t = t.expand((1)).to(device)\n",
    "                a_t = CosineNoise.get_alpha_t(t)\n",
    "                alpha_t_barre = CosineNoise.get_alpha_t_barre(t)\n",
    "                sigma = torch.sqrt(1-a_t).view(1, 1, 1, 1)\n",
    "                noise = torch.randn_like(xt)\n",
    "                # print(xt.shape)\n",
    "                label = torch.tensor([i%10], device=device)\n",
    "                epsilon = model(xt, t, label)\n",
    "                \n",
    "                a = ((1 - a_t)/(torch.sqrt(1 - alpha_t_barre))).view(1, 1, 1, 1)\n",
    "                b = (1/torch.sqrt(a_t)).view(1, 1, 1, 1)\n",
    "                \n",
    "                if t.item() % (max_time_steps / 10) == 0 or t.item() == max_time_steps-1:\n",
    "                    # print(t.item())\n",
    "                    full_img = torch.cat((full_img, xt), 3)\n",
    "                    full_predicted_noise = torch.cat((full_predicted_noise, epsilon), 3)\n",
    "                    # print(xt.shape)\n",
    "                    # show_image(xt[0], f'{t.item()}%')\n",
    "                    # show_image(full_img[0])\n",
    "                \n",
    "                xt = b*(xt - a*epsilon) + sigma*noise\n",
    "\n",
    "                # xt = torch.sqrt(1 - a_t).view(1, 1, 1, 1) * noise_predicted + sigma * noise\n",
    "                \n",
    "                # xt = b * (xt - torch.sqrt(1-alpha_t_barre)*noise_predicted) + sigma*noise\n",
    "\n",
    "        \n",
    "            all_samples.append(xt[0].cpu())\n",
    "            all_vis.append(full_img[0].cpu())\n",
    "        \n",
    "            # show_image(xt[0], title=f'Final Image of {label.item()}')\n",
    "            # show_image(full_img[0])\n",
    "            \n",
    "    return all_samples, all_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time_steps = 200\n",
    "n_samples = 10\n",
    "\n",
    "imgs, full_imgs = sample(model, max_time_steps, n_samples, config_CELEBA, device=device)\n",
    "for i, (xt, full_img) in enumerate(zip(imgs, full_imgs)):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(xt.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "    plt.title(f\"Final Image of {i}\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(full_img.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "    plt.title(f\"Sampling process of {i}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
